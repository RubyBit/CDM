{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NYU_5HgbBXKX",
    "outputId": "9d13f9db-5d25-47a4-ba70-737a3b6a8348"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchsummary in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (3.7.4.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2020.12.5)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torchsummary\n",
    "pip install torchvision \n",
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "#from einops import rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "\tdef __init__(self, encoded_space_dim):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t### Convolutional section\n",
    "\t\tself.encoder_cnn = nn.Sequential(\n",
    "\t\tnn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
    "\t\tnn.ReLU(True),\n",
    "\t\tnn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
    "\t\tnn.BatchNorm2d(16),\n",
    "\t\tnn.ReLU(True),\n",
    "\t\tnn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
    "\t\tnn.ReLU(True)\n",
    "\t\t)\n",
    "\n",
    "\t\t### Flatten layer\n",
    "\t\tself.flatten = nn.Flatten(start_dim=1)\n",
    "### Linear section\n",
    "\t\tself.encoder_lin = nn.Sequential(\n",
    "\t\t\tnn.Linear(3 * 3 * 32, 128),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(128, encoded_space_dim)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.encoder_cnn(x)\n",
    "\t\tx = self.flatten(x)\n",
    "\t\tx = self.encoder_lin(x)\n",
    "\t\treturn x\n",
    "\t#output is of dimension (1, encoded_space_dim)\n",
    "class Decoder(nn.Module):\n",
    "\tdef __init__(self, encoded_space_dim):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.decoder_lin = nn.Sequential(\n",
    "\t\t\tnn.Linear(encoded_space_dim, 128),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.Linear(128, 3 * 3 * 32),\n",
    "\t\t\tnn.ReLU(True)\n",
    "\t\t)\n",
    "\n",
    "\t\tself.unflatten = nn.Unflatten(dim=1,\n",
    "\t\tunflattened_size=(32, 3, 3))\n",
    "\n",
    "\t\tself.decoder_conv = nn.Sequential(\n",
    "\t\t\tnn.ConvTranspose2d(32, 16, 3,\n",
    "\t\t\tstride=2, output_padding=0),\n",
    "\t\t\tnn.BatchNorm2d(16),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.ConvTranspose2d(16, 8, 3, stride=2,\n",
    "\t\t\tpadding=1, output_padding=1),\n",
    "\t\t\tnn.BatchNorm2d(8),\n",
    "\t\t\tnn.ReLU(True),\n",
    "\t\t\tnn.ConvTranspose2d(8, 1, 3, stride=2,\n",
    "\t\t\tpadding=1, output_padding=1)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.decoder_lin(x)\n",
    "\t\tx = self.unflatten(x)\n",
    "\t\tx = self.decoder_conv(x)\n",
    "\t\tx = torch.sigmoid(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), \n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), \n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2),\n",
    "            act_fn(),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels : int,\n",
    "                 base_channel_size : int,\n",
    "                 latent_dim : int,\n",
    "                 act_fn : object = nn.GELU):\n",
    "\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid),\n",
    "            act_fn()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), \n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=0),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            nn.Tanh() \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 128])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = Encoder(1, 32,128)\n",
    "x        = torch.randn(1000, 1, 28, 28)\n",
    "enc(x).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, num_blocks=4, num_layers=4, num_filters=64, kernel_size=3, stride=1, padding=1,\n",
    "                 dilation=1, groups=1, bias=True, padding_mode='zeros', activation=nn.ReLU, norm=nn.BatchNorm2d,\n",
    "                 dropout=nn.Dropout2d, residual=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        self.activation = activation\n",
    "        self.norm = norm\n",
    "        self.dropout = dropout\n",
    "        self.in_ch = in_ch\n",
    "        self.out_ch = out_ch\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.bias = bias\n",
    "        self.padding_mode = padding_mode\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "        self.blocks = nn.ModuleList([self._make_block() for _ in range(self.num_blocks)])\n",
    "        self.head = nn.Conv2d(self.num_filters, self.out_ch, 1)\n",
    "\n",
    "    def _make_block(self):\n",
    "        layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            layers.append(nn.Conv2d(self.in_ch, self.num_filters, self.kernel_size, self.stride, self.padding,\n",
    "                                    self.dilation, self.groups, self.bias, self.padding_mode))\n",
    "            if self.norm is not None:\n",
    "                layers.append(self.norm(self.num_filters))\n",
    "            if self.activation is not None:\n",
    "                layers.append(self.activation())\n",
    "            if self.dropout is not None:\n",
    "                layers.append(self.dropout())\n",
    "            self.in_ch = self.num_filters\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cond=None):\n",
    "        for block in self.blocks:\n",
    "            res = x\n",
    "            x = block(x)\n",
    "            if cond is not None:\n",
    "                x += nn.Linear(cond.shape[1], x.shape[1], bias=False)(cond)\n",
    "            if self.residual:\n",
    "                x = x + res\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Score neural network for the diffusion process. Approximates what you should do at each timestep\n",
    "class ScoreNet(nn.Module):\n",
    "\n",
    "    def __init__(self, latent_dim, embedding_dim, n_blocks=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.resnet = ResNet(self.latent_dim, self.latent_dim, num_blocks=n_blocks, num_layers=4,\n",
    "                             num_filters=64, kernel_size=1, stride=1, padding=1, dilation=1, groups=1, bias=True,\n",
    "                             padding_mode='zeros', activation=nn.ReLU, norm=nn.BatchNorm2d, dropout=nn.Dropout2d,\n",
    "                             residual=True)\n",
    "\n",
    "    def forward(self, x, t, conditioning):\n",
    "        \n",
    "        timestep = get_timestep_embedding(t, self.embedding_dim)\n",
    "        print(\"Timstep dim {}, Cond dim {}\",timestep.shape,  conditioning.shape)\n",
    "        cond = torch.cat([timestep, conditioning], dim=1)\n",
    "        #cond=nn.Flatten(0)(cond)\n",
    "        print(\"Input to liearn\",cond.shape)\n",
    "        \n",
    "        cond = nn.SiLU()(nn.Linear(288, self.embedding_dim * 4)(cond))\n",
    "        cond = nn.SiLU()(nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4)(cond))\n",
    "        cond = nn.Linear(self.embedding_dim * 4, self.embedding_dim)(cond)\n",
    "        print(x.shape)\n",
    "        #x=nn.Flatten(0)(x)\n",
    "        print(x.shape)\n",
    "        h = nn.Linear(self.latent_dim, self.embedding_dim)(x)\n",
    "        h = self.resnet(h, cond)\n",
    "        return x + h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cond=nn.rand()\n",
    "cond=nn.Flatten()(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    assert len(timesteps.shape) == 1\n",
    "    timesteps *= 1000\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = np.exp(np.arange(half_dim) * -emb)\n",
    "    emb = np.outer(timesteps, emb)\n",
    "    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n",
    "    print(timesteps.shape, embedding_dim)\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return torch.from_numpy(emb).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,) 32\n",
      "Timstep dim {}, Cond dim {} torch.Size([128, 32]) torch.Size([128, 256])\n",
      "Input to liearn torch.Size([128, 288])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c2c5361a4368>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mg_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mscorenet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconditioning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1c92c6d9f6d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, t, conditioning)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-1c92c6d9f6d0>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, cond)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcond\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                 \u001b[0mx\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 459\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [1, 32]"
     ]
    }
   ],
   "source": [
    "# Testing whole scorenet\n",
    "Time=4\n",
    "def gamma(ts, gamma_min=-6, gamma_max=6):\n",
    "    return gamma_max + (gamma_min - gamma_max) * ts\n",
    "g_t = gamma(Time)\n",
    "embed=256\n",
    "conditioning = torch.arange(128) % (10 + 26 + 26 + 1)\n",
    "conditioning=torch.nn.Embedding( num_embeddings=128,embedding_dim=embed)(conditioning)\n",
    "scorenet = ScoreNet(latent_dim=128,embedding_dim=32)\n",
    "x        = torch.randn(1,128)\n",
    "#setting the t to be a vector\n",
    "t= g_t * np.ones(x.shape[1])\n",
    "\n",
    "scorenet(x,t,conditioning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing dimension match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,) 32\n",
      "torch.Size([128, 32]) torch.Size([128, 32])\n"
     ]
    }
   ],
   "source": [
    "# Stesting get_timestep_embedding\n",
    "Time=4\n",
    "def gamma(ts, gamma_min=-6, gamma_max=6):\n",
    "    return gamma_max + (gamma_min - gamma_max) * ts\n",
    "g_t = gamma(Time)\n",
    "\n",
    "\n",
    "#Dimneison of conditioning must be same as th second dimenison of the x in this case\n",
    "conditioning = torch.arange(128) % (10 + 26 + 26 + 1)\n",
    "conditioning=torch.nn.Embedding( num_embeddings=10 + 26 + 26 + 1,embedding_dim=32)(conditioning)\n",
    "scorenet = ScoreNet(latent_dim=128,embedding_dim=128)\n",
    "x        = torch.randn(1,128)#encoder output\n",
    "\n",
    "#Setting the t to be a vector\n",
    "t= g_t * np.ones(x.shape[1])\n",
    "timestep=get_timestep_embedding(t, 32)\n",
    "print(timestep.shape,  conditioning.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 128])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditioning = torch.arange(256) % (10 + 26 + 26 + 1)\n",
    "conditioning=torch.nn.Embedding( num_embeddings=10 + 26 + 26 + 1,embedding_dim=128)(conditioning)\n",
    "conditioning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond       = torch.randn(10,128)\n",
    "cond=nn.Flatten(0)(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128,) 128\n",
      "Timstep dim {}, Cond dim {} torch.Size([128, 128]) torch.Size([128, 256])\n",
      "Input to liearn torch.Size([49152])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x128 and 49152x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-123526ba6d44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mg_t\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mscorenet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconditioning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-62-73c5916ddf66>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, t, conditioning)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mcond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dim\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x128 and 49152x128)"
     ]
    }
   ],
   "source": [
    "# Testing whole scorenet\n",
    "Time=4\n",
    "def gamma(ts, gamma_min=-6, gamma_max=6):\n",
    "    return gamma_max + (gamma_min - gamma_max) * ts\n",
    "g_t = gamma(Time)\n",
    "embed=256\n",
    "latent=49152\n",
    "conditioning = torch.arange(128) % (10 + 26 + 26 + 1)\n",
    "conditioning=torch.nn.Embedding( num_embeddings=128,embedding_dim=embed)(conditioning)\n",
    "scorenet = ScoreNet(latent_dim=latent,embedding_dim=128)\n",
    "x        = torch.randn(1,128)\n",
    "#setting the t to be a vector\n",
    "t= g_t * np.ones(x.shape[1])\n",
    "\n",
    "scorenet(x,t,conditioning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditioning = torch.arange(128) % (10 + 26 + 26 + 1)\n",
    "conditioning.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting livelossplot\n",
      "  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: bokeh in c:\\programdata\\anaconda3\\lib\\site-packages (from livelossplot) (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from livelossplot) (3.3.4)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (5.4.1)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (20.9)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (2.11.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (1.20.1)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (2.8.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (8.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\programdata\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh->livelossplot) (1.1.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=16.8->bokeh->livelossplot) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (0.10.0)\n",
      "Installing collected packages: livelossplot\n",
      "Successfully installed livelossplot-0.5.5\n"
     ]
    }
   ],
   "source": [
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "# from einops import rearrange\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "# https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels: int,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2 * 16 * c_hid, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_input_channels: int,\n",
    "                 base_channel_size: int,\n",
    "                 latent_dim: int):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2 * 16 * c_hid),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * c_hid, 2 * c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(2 * c_hid, 2 * c_hid, kernel_size=3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(2 * c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=0),\n",
    "            nn.GELU(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(torch.float32)  # because linear layer expects float32\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "encoder = Encoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
    "# input image\n",
    "x = torch.randn(10000, 1, 28, 28)\n",
    "encoder(x).shape\n",
    "decoder = Decoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
    "# input image\n",
    "x = torch.randn(1000, 256)\n",
    "decoder(x).shape\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Measures the reconstruction loss from the encoding the image to latent space and then decoding it back to the image\n",
    "def autoencoder_loss(x, x_hat):\n",
    "    return F.binary_cross_entropy(x_hat, x)  # For MNIST dataset (or log prob if we get distributions)\n",
    "\n",
    "\n",
    "# Latent loss\n",
    "def latent_loss(x_hat):\n",
    "    var_1 = sigma2(gamma(x_hat))\n",
    "    mean1_sqr = (1.0 - var_1) * torch.square(x_hat)\n",
    "    #print((mean1_sqr + var_1 - np.log(var_1)).shape)\n",
    "    loss_lat = 0.5 * torch.mean(mean1_sqr + var_1 - np.log(var_1) - 1.0)\n",
    "    return loss_lat\n",
    "\n",
    "\n",
    "def recon_loss(img, enc_img, decoder: Decoder):\n",
    "    g_0 = gamma(0)\n",
    "    # numpy normal distribution\n",
    "    eps_0 = np.random.normal(size=enc_img.size())\n",
    "    z_0 = variance_map(enc_img, g_0, eps_0)\n",
    "    # rescale\n",
    "    z_0_rescaled = z_0 / alpha(g_0)\n",
    "    # decode\n",
    "    decoded_img = decoder(z_0_rescaled)\n",
    "    # make sure decoded_img is positive (change tensor sign)\n",
    "    decoded_img = torch.where(decoded_img < 0, -decoded_img, decoded_img)  # this might be a bottleneck\n",
    "    return autoencoder_loss(img, decoded_img)\n",
    "\n",
    "\n",
    "############################################################################################################\n",
    "# Diffusion process functions\n",
    "############################################################################################################\n",
    "# The timestep embedding is for the diffusion model to learn the temporal information of the time series\n",
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    timesteps *= 1000\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = np.log(10000) / (half_dim - 1)\n",
    "    emb = np.exp(np.arange(half_dim) * -emb)\n",
    "    emb = np.outer(timesteps, emb)\n",
    "    emb = np.concatenate([np.sin(emb), np.cos(emb)], axis=1)\n",
    "\n",
    "    assert emb.shape == (timesteps.shape[0], embedding_dim)\n",
    "    return torch.from_numpy(emb).float()\n",
    "\n",
    "\n",
    "# Forward diffusion process functions\n",
    "def gamma(ts, gamma_min=-6, gamma_max=6):\n",
    "    return gamma_max + (gamma_min - gamma_max) * ts\n",
    "\n",
    "\n",
    "def sigma2(gamma_x):\n",
    "    tensor = torch.tensor(gamma_x)\n",
    "    return torch.sigmoid(-tensor)  # correct?\n",
    "\n",
    "\n",
    "def alpha(gamma_x):\n",
    "    return np.sqrt(1 - sigma2(gamma_x))\n",
    "\n",
    "\n",
    "def variance_map(x, gamma_x, eps):\n",
    "    return alpha(gamma_x) * x + np.sqrt(sigma2(gamma_x)) * eps\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    # Residual network\n",
    "    def __init__(self, latent_dim, embed_dim, num_blocks=4, num_layers=10, activation=nn.ReLU, norm=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "        self.norm = norm\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for _ in range(self.num_layers):\n",
    "            self.blocks.append(self._make_block())\n",
    "\n",
    "    def _make_block(self):\n",
    "        # without convolutional layers\n",
    "        layers = [self.norm([self.latent_dim]), self.activation(), nn.Linear(self.latent_dim, self.embed_dim)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, cond):\n",
    "        z = x\n",
    "        for block in self.blocks:\n",
    "            h = block(z)\n",
    "            if cond is not None:\n",
    "                h = h + nn.Linear(cond.shape[1], self.embed_dim, bias=False)(cond)\n",
    "            h = self.activation()(self.norm([self.embed_dim])(h))\n",
    "            h = nn.Linear(self.embed_dim, self.latent_dim)(h)\n",
    "        z = z + h\n",
    "        return z\n",
    "\n",
    "\n",
    "# Score neural network for the diffusion process. Approximates what you should do at each timestep\n",
    "class ScoreNet(nn.Module):\n",
    "    def __init__(self, latent_dim, embedding_dim, n_blocks=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.resnet = ResNet(self.embedding_dim, self.embedding_dim * 2)\n",
    "\n",
    "    def forward(self, x, t, conditioning):\n",
    "        #print(\" T.dim={}, X.dim={}\".format(t.shape, x.shape))\n",
    "        timestep = get_timestep_embedding(t, self.embedding_dim)\n",
    "        # assert conditioning.shape[0]==timestep.shape[0] #as the output of encoder is (1, encoded_dim) this condition must eb satisfied\n",
    "        cond = timestep  # cond = torch.cat((timestep, conditioning), dim=1)\n",
    "        #print(cond.shape)\n",
    "        cond = nn.SiLU()(nn.Linear(self.embedding_dim, self.embedding_dim * 4)(cond))\n",
    "        cond = nn.SiLU()(nn.Linear(self.embedding_dim * 4, self.embedding_dim * 4)(cond))\n",
    "        cond = nn.Linear(self.embedding_dim * 4, self.embedding_dim)(cond)\n",
    "\n",
    "        h = nn.Linear(self.latent_dim, self.embedding_dim)(x)  # hardcoded but should be latent_dim\n",
    "        #h = torch.reshape(h, (1, 32, 1, 1))  # Reshaped for convolutional layers\n",
    "        h = self.resnet(h, cond)\n",
    "        return x + h\n",
    "\n",
    "\n",
    "def diffusion_loss(z_0, t, score_net, conditioning):\n",
    "    # z_0 is the initial latent variable\n",
    "    # t is the time step (time steps need to be discrete)\n",
    "    # z_t is the latent variable at time t\n",
    "    # z_t is a function of z_0 and t\n",
    "\n",
    "    eps = torch.randn_like(z_0)\n",
    "    gamma_x = gamma(t)\n",
    "    z_t = variance_map(z_0, gamma_x, eps)\n",
    "\n",
    "    # The score function is the derivative of the latent variable with respect to time\n",
    "    score = score_net(z_t, t, conditioning)\n",
    "    loss_diff_mse = torch.mean((score - z_t) ** 2)\n",
    "\n",
    "    # The diffusion process is a stochastic process\n",
    "    T = len(t)\n",
    "    s = t - (1. / T)\n",
    "    g_s = gamma(s)\n",
    "    loss_diff = 0.5 * torch.mean(np.expm1(g_s - gamma_x) * loss_diff_mse)\n",
    "\n",
    "    return loss_diff\n",
    "\n",
    "\n",
    "class VariationalDiffusion(nn.Module):\n",
    "    timesteps: int = 1000\n",
    "    layers: int = 32\n",
    "    gamma_min: float = -3.0\n",
    "    gamma_max: float = 3.0\n",
    "\n",
    "    def __init__(self, latent_dim, embedding_dim, n_blocks=32):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.score_net = ScoreNet(self.latent_dim, self.embedding_dim, n_blocks=n_blocks)\n",
    "        self.encoder = Encoder(1, 32, 128)\n",
    "        self.decoder = Decoder(1, 32, 128)\n",
    "\n",
    "    def forward(self, img, conditioning=None):  # combined loss for diffusion and reconstruction\n",
    "        # encoding image\n",
    "        z_0 = self.encoder(img)\n",
    "        # encoder loss\n",
    "        loss_recon = recon_loss(img, z_0, self.decoder)\n",
    "\n",
    "        loss_latent = latent_loss(z_0)\n",
    "\n",
    "        # diffusion loss\n",
    "        # we need to sample time steps\n",
    "        t = torch.rand((z_0.shape[0], 1))\n",
    "        # discretize time steps\n",
    "        t = np.ceil(t * self.timesteps)\n",
    "        loss_diff = diffusion_loss(z_0, t, self.score_net, conditioning)\n",
    "        return loss_recon + loss_latent - loss_diff, (loss_recon, loss_latent, loss_diff)\n",
    "\n",
    "    def sample(self, z, t, conditioning, num_samples=1):\n",
    "        eps = torch.randn((num_samples, self.latent_dim))\n",
    "        gamma_x = gamma(t)\n",
    "        z_t = variance_map(eps, gamma_x, eps)\n",
    "        score = self.score_net(z_t, t, conditioning)\n",
    "        return z_t + score\n",
    "\n",
    "    def sample_from_prior(self, t, num_samples=1):\n",
    "        return self.sample(t, conditioning=torch.zeros((num_samples, 0)), num_samples=num_samples)\n",
    "\n",
    "    def sample_from_posterior(self, t, conditioning, num_samples=1):\n",
    "        return self.sample(t, conditioning=conditioning, num_samples=num_samples)\n",
    "def TrainVDM(batch_size_train=100, epoch=4):\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('../', train=True, download=False,\n",
    "                                transform=torchvision.transforms.Compose([\n",
    "                                torchvision.transforms.ToTensor(),\n",
    "                                torchvision.transforms.Normalize(\n",
    "                                    (0.1307,), (0.3081,))\n",
    "                                ])),batch_size=batch_size_train, shuffle=True)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model=VariationalDiffusion(128, 128).to(device)\n",
    "    model.train()\n",
    "    log_interval=50\n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    loss_f= torch.nn.MSELoss()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "        optimizer.zero_grad()\n",
    "        loss, values = model(data)\n",
    "        #print(values)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))\n",
    "            train_losses.append(loss.item())\n",
    "            train_counter.append(\n",
    "            (batch_idx*1000) + ((epoch-1)*len(train_loader.dataset)))\n",
    "if __name__ == \"__main__\":\n",
    "    # model\n",
    "    model = VariationalDiffusion(latent_dim=128, embedding_dim=128)\n",
    "    # a random image 28x28x1\n",
    "    img = torch.randn(1, 1, 28, 28)\n",
    "    #loss, values = model(img)\n",
    "    #print(values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-b339dfa9bc00>:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tensor = torch.tensor(gamma_x)\n",
      "<ipython-input-12-b339dfa9bc00>:103: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_lat = 0.5 * torch.mean(mean1_sqr + var_1 - np.log(var_1) - 1.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.870295\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 101.399567\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 102.708389\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 98.946945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-44f4e0a3036d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTrainVDM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-b339dfa9bc00>\u001b[0m in \u001b[0;36mTrainVDM\u001b[1;34m(batch_size_train, epoch)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m#print(values)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "TrainVDM()\n",
    "torch.save(model.state_dict(), \"./MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
