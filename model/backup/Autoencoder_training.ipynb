{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYU_5HgbBXKX",
        "outputId": "9d13f9db-5d25-47a4-ba70-737a3b6a8348"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchsummary in c:\\programdata\\anaconda3\\lib\\site-packages (1.5.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchvision in c:\\programdata\\anaconda3\\lib\\site-packages (0.14.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (2.25.1)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.20.1)\n",
            "Requirement already satisfied: torch==1.13.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (1.13.1)\n",
            "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (3.7.4.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torchvision) (8.2.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->torchvision) (2.10)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install torchvision "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aMU344OHhk5D"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "#from einops import rearrange\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from torchvision import transforms\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "enc_block = Block(1, 64)\n",
        "x         = torch.randn(1, 1, 28, 28)\n",
        "enc_block(x).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJJnzepW8V8h"
      },
      "outputs": [],
      "source": [
        "#https://github.com/g2archie/UNet-MRI-Reconstruction\n",
        "#https://amaarora.github.io/2020/09/13/unet.html#understanding-input-and-output-shapes-in-u-net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z05n8evCoj_0"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.relu  = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.relu(self.conv2(self.relu(self.conv1(x))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uCmG6Z_ol6z",
        "outputId": "4387d7ce-c7c6-490a-830b-b7bcc6f978da"
      },
      "outputs": [],
      "source": [
        "enc_block = Block(1, 64)\n",
        "x         = torch.randn(1, 1, 28, 28)\n",
        "enc_block(x).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHMVsckgoejc"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, chs=(1,32,64,128,256)):\n",
        "        super().__init__()\n",
        "        self.enc_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])\n",
        "        self.pool       = nn.MaxPool2d(2)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        ftrs = []\n",
        "        for block in self.enc_blocks:\n",
        "            x = block(x)\n",
        "            ftrs.append(x)\n",
        "            x = self.pool(x)\n",
        "        return ftrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ToF3F2kP27To",
        "outputId": "a602ec82-021a-4003-f689-5e529143463a"
      },
      "outputs": [],
      "source": [
        "chs=(1,32,64,128,256)\n",
        "nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG3chmc9q_Eb",
        "outputId": "e4b8a28a-c88f-4d86-abc0-5806243f5a6d"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder()\n",
        "# input image\n",
        "x    = torch.randn(1, 1, 28, 28)\n",
        "ftrs = encoder(x)\n",
        "for ftr in ftrs: print(ftr.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(1024,512, 256, 128, 64)):\n",
        "        super().__init__()\n",
        "        self.chs         = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
        "        \n",
        "    def forward(self, x):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RzVrFvLKrDNn"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, chs=(256, 128, 64, 32, 1)):\n",
        "        super().__init__()\n",
        "        self.chs         = chs\n",
        "        self.upconvs    = nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])\n",
        "        self.dec_blocks = nn.ModuleList([Block(chs[i], chs[i+1]) for i in range(len(chs)-1)]) \n",
        "        \n",
        "    def forward(self, x, encoder_features=ftrs[::-1][1:]):\n",
        "        for i in range(len(self.chs)-1):\n",
        "            x        = self.upconvs[i](x)\n",
        "            enc_ftrs = self.crop(encoder_features[i], x)\n",
        "            x        = torch.cat([x, enc_ftrs], dim=1)\n",
        "            x        = self.dec_blocks[i](x)\n",
        "        return x\n",
        "    \n",
        "    def crop(self, enc_ftrs, x):\n",
        "        _, _, H, W = x.shape\n",
        "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
        "        return enc_ftrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "model = Decoder().to(device)\n",
        "x=torch.randn(1, 256, 3, 3)\n",
        "summary(model,(1, 256, 3, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X3jtew2tbco",
        "outputId": "e5125b14-4358-4091-bb0c-1c003b301815"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder()\n",
        "x = torch.randn(1, 256, 3, 3)\n",
        "decoder(x,ftrs[::-1][1:]).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2bng6y__EQE",
        "outputId": "c5cdfb57-b765-4096-915e-e394086ff7b5"
      },
      "outputs": [],
      "source": [
        "chs=(256, 128, 64,32,1)\n",
        "nn.ModuleList([nn.ConvTranspose2d(chs[i], chs[i+1], 2, 2) for i in range(len(chs)-1)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stsXUxpwvH8Y"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "  def __init__(self, enc_chs=(1,32,64,128,256), dec_chs=(256, 128, 64, 32), num_class=1, retain_dim=False, out_sz=(572,572)):\n",
        "      super().__init__()\n",
        "      self.encoder     = Encoder(enc_chs)\n",
        "      self.decoder     = Decoder(dec_chs)\n",
        "      self.head        = nn.Conv2d(dec_chs[-1], num_class, 1)\n",
        "      self.retain_dim  = retain_dim\n",
        "\n",
        "  def forward(self, x):\n",
        "      enc_ftrs = self.encoder(x)\n",
        "      out      = self.decoder(enc_ftrs[::-1][0], enc_ftrs[::-1][1:])\n",
        "      out      = self.head(out)\n",
        "      if self.retain_dim:\n",
        "          out = F.interpolate(out, out_sz)\n",
        "      return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kgGcnyOAwUB",
        "outputId": "0a2591e7-a01c-4407-cc44-cc10a74b0895"
      },
      "outputs": [],
      "source": [
        "import torchsummary\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
        "model = UNet().to(device)\n",
        "\n",
        "summary(model, (1, 1, 256, 256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoKKfamr-VUr",
        "outputId": "c2625149-43df-46f8-baf0-ed9060f1cccd"
      },
      "outputs": [],
      "source": [
        "unet = UNet()\n",
        "x    = torch.randn(1, 1, 256, 256)\n",
        "unet(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "\tdef __init__(self, encoded_space_dim):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\t### Convolutional section\n",
        "\t\tself.encoder_cnn = nn.Sequential(\n",
        "\t\tnn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "\t\tnn.ReLU(True),\n",
        "\t\tnn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "\t\tnn.BatchNorm2d(16),\n",
        "\t\tnn.ReLU(True),\n",
        "\t\tnn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "\t\tnn.ReLU(True)\n",
        "\t\t)\n",
        "\n",
        "\t\t### Flatten layer\n",
        "\t\tself.flatten = nn.Flatten(start_dim=1)\n",
        "### Linear section\n",
        "\t\tself.encoder_lin = nn.Sequential(\n",
        "\t\t\tnn.Linear(3 * 3 * 32, 128),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.Linear(128, encoded_space_dim)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.encoder_cnn(x)\n",
        "\t\tx = self.flatten(x)\n",
        "\t\tx = self.encoder_lin(x)\n",
        "\t\treturn x\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "\tdef __init__(self, encoded_space_dim):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.decoder_lin = nn.Sequential(\n",
        "\t\t\tnn.Linear(encoded_space_dim, 128),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.Linear(128, 3 * 3 * 32),\n",
        "\t\t\tnn.ReLU(True)\n",
        "\t\t)\n",
        "\n",
        "\t\tself.unflatten = nn.Unflatten(dim=1,\n",
        "\t\tunflattened_size=(32, 3, 3))\n",
        "\n",
        "\t\tself.decoder_conv = nn.Sequential(\n",
        "\t\t\tnn.ConvTranspose2d(32, 16, 3,\n",
        "\t\t\tstride=2, output_padding=0),\n",
        "\t\t\tnn.BatchNorm2d(16),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.ConvTranspose2d(16, 8, 3, stride=2,\n",
        "\t\t\tpadding=1, output_padding=1),\n",
        "\t\t\tnn.BatchNorm2d(8),\n",
        "\t\t\tnn.ReLU(True),\n",
        "\t\t\tnn.ConvTranspose2d(8, 1, 3, stride=2,\n",
        "\t\t\tpadding=1, output_padding=1)\n",
        "\t\t)\n",
        "\n",
        "\tdef forward(self, x):\n",
        "\t\tx = self.decoder_lin(x)\n",
        "\t\tx = self.unflatten(x)\n",
        "\t\tx = self.decoder_conv(x)\n",
        "\t\tx = torch.sigmoid(x)\n",
        "\t\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "decoder = Encoder(encoded_space_dim=64)\n",
        "x = torch.randn(1, 1, 28, 28)\n",
        "Encoder(encoded_space_dim=64)(x).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "batch_size = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def autoencoder_loss(x, x_hat):\n",
        "    return F.binary_cross_entropy(x_hat, x) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#https://gist.github.com/Mahedi-61/e70f08e1f36aa9a4fa575d2a5a3f6c25\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#n_epochs = 3\n",
        "#batch_size_train = 64\n",
        "#batch_size_test = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./', train=True, download=False,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('./', train=False, download=False,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "example_targets.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "\n",
        "for i in range(6):\n",
        "    plt.subplot(2,3,i+1)\n",
        "    plt.tight_layout()\n",
        "    plt.imshow(example_data[i][0], cmap='gray', interpolation='none')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "def shortcut(ims):\n",
        "\n",
        "    \n",
        "    f = self.encoder(ims, cond)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([10000, 256])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial9/AE_CIFAR10.html\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_input_channels : int,\n",
        "                 base_channel_size : int,\n",
        "                 latent_dim : int,\n",
        "                 act_fn : object = nn.GELU):\n",
        "\n",
        "        super().__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), \n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), \n",
        "            act_fn(),\n",
        "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2),\n",
        "            act_fn(),\n",
        "            nn.Flatten(), \n",
        "            nn.Linear(2*16*c_hid, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_input_channels : int,\n",
        "                 base_channel_size : int,\n",
        "                 latent_dim : int,\n",
        "                 act_fn : object = nn.GELU):\n",
        "\n",
        "        super().__init__()\n",
        "        c_hid = base_channel_size\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 2*16*c_hid),\n",
        "            act_fn()\n",
        "        )\n",
        "        self.net = nn.Sequential(\n",
        "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1),\n",
        "            act_fn(),\n",
        "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), \n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=0),\n",
        "            act_fn(),\n",
        "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2),\n",
        "            nn.Tanh() \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x)\n",
        "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
        "        x = self.net(x)\n",
        "        return x\n",
        "encoder = Encoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
        "# input image\n",
        "x    = torch.randn(10000,1, 28, 28)\n",
        "encoder(x).shape\n",
        "decoder = Decoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
        "# input image\n",
        "#x    = torch.randn(1000,256)\n",
        "#decoder(x).shape\n",
        "encoder(x).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train(epoch, train_loader, optimizer, encoder, decoder):\n",
        "    log_interval=50\n",
        "    train_losses = []\n",
        "    train_counter = []\n",
        "    loss_f= torch.nn.MSELoss()\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        encoded_data = encoder(data)\n",
        "        # Decode data\n",
        "        decoded_data = decoder(encoded_data)\n",
        "        loss = loss_f(decoded_data, data)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "            100. * batch_idx / len(train_loader), loss.item()))\n",
        "            train_losses.append(loss.item())\n",
        "            train_counter.append(\n",
        "            (batch_idx*1000) + ((epoch-1)*len(train_loader.dataset)))\n",
        "def test(test_loader, encoder, decoder):\n",
        "    loss_f= torch.nn.MSELoss()\n",
        "    test_losses = []\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            encoded_data = encoder(data)\n",
        "            # Decode data\n",
        "            output= decoder(encoded_data)\n",
        "            test_loss += loss_f(output,data).item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    test_losses.append(test_loss)\n",
        "    print('\\nTest set: Avg. loss: {:.4f} \\n'.format(\n",
        "        test_loss))\n",
        "    #output image plotting\n",
        "    \n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()\n",
        "\n",
        "def short_cut(n_epochs, batch_size_train,batch_size_test):\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./', train=True, download=False,\n",
        "                                transform=torchvision.transforms.Compose([\n",
        "                                torchvision.transforms.ToTensor(),\n",
        "                                torchvision.transforms.Normalize(\n",
        "                                    (0.1307,), (0.3081,))\n",
        "                                ])),batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "    torchvision.datasets.MNIST('./', train=False, download=False,\n",
        "                                transform=torchvision.transforms.Compose([\n",
        "                                torchvision.transforms.ToTensor(),\n",
        "                                torchvision.transforms.Normalize(\n",
        "                                    (0.1307,), (0.3081,))\n",
        "                                ])),batch_size=batch_size_test, shuffle=True)\n",
        "\n",
        "    encoder=Encoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
        "    decoder=Decoder(num_input_channels=1, base_channel_size=32, latent_dim=256)\n",
        "    mean = (0.1307, )\n",
        "    std = (0.3081, ) \n",
        "    learning_rate = 0.01\n",
        "\n",
        "    params_to_optimize = [\n",
        "        {'params': encoder.parameters()},\n",
        "        {'params': decoder.parameters()}\n",
        "    ]\n",
        "    optimizer = torch.optim.Adam(params_to_optimize,lr=learning_rate)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        train(epoch=epoch, train_loader=train_loader, optimizer=optimizer, encoder=encoder,decoder=decoder)\n",
        "        test(test_loader=test_loader, encoder=encoder,decoder=decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.955623\n",
            "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 1.989986\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.993842\n",
            "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 1.977072\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.977300\n",
            "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 1.963278\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.954767\n",
            "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 2.006687\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.986012\n",
            "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 1.988836\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.032730\n",
            "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 2.029821\n",
            "\n",
            "Test set: Avg. loss: 0.0202 \n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.960022\n",
            "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 2.026124\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 1.958747\n",
            "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 1.983825\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.976848\n",
            "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 1.972339\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 1.983077\n",
            "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 1.986896\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.020645\n",
            "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 2.004883\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.006372\n",
            "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 2.008822\n",
            "\n",
            "Test set: Avg. loss: 0.0202 \n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.003948\n",
            "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 2.015588\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 1.951326\n",
            "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 1.962039\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.990916\n",
            "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 2.095232\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.040434\n",
            "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 1.995532\n",
            "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.009871\n",
            "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 1.981442\n",
            "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.997349\n",
            "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 2.038652\n",
            "\n",
            "Test set: Avg. loss: 0.0202 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "short_cut(3,100,100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
